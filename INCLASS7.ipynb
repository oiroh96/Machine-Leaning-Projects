{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKSTLF2BX6jH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N11Ee3GJmywu",
    "outputId": "64e10b54-dad3-48e2-c266-617670a17e82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/26/a1/75474477af2a1dae3a25f80b72bbaf20e8296191ece7fff2f67984206f33/openai-1.12.0-py3-none-any.whl.metadata\n",
      "  Downloading openai-1.12.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /Users/OscarIroh_1/anaconda3/lib/python3.11/site-packages (from openai) (3.5.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Obtaining dependency information for distro<2,>=1.7.0 from https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl.metadata\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Obtaining dependency information for httpx<1,>=0.23.0 from https://files.pythonhosted.org/packages/41/7b/ddacf6dcebb42466abd03f368782142baa82e08fc0c1f8eaa05b4bae87d5/httpx-0.27.0-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/OscarIroh_1/anaconda3/lib/python3.11/site-packages (from openai) (1.10.8)\n",
      "Requirement already satisfied: sniffio in /Users/OscarIroh_1/anaconda3/lib/python3.11/site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/OscarIroh_1/anaconda3/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/OscarIroh_1/anaconda3/lib/python3.11/site-packages (from openai) (4.7.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/OscarIroh_1/anaconda3/lib/python3.11/site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/OscarIroh_1/anaconda3/lib/python3.11/site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/OscarIroh_1/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/OscarIroh_1/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/2c/93/13f25f2f78646bab97aee7680821e30bd85b2ff0fc45d5fdf5393b79716d/httpcore-1.0.4-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Obtaining dependency information for h11<0.15,>=0.13 from https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl.metadata\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/OscarIroh_1/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/OscarIroh_1/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.16)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/OscarIroh_1/anaconda3/lib/python3.11/site-packages (from beautifulsoup4->wikipedia) (2.4)\n",
      "Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=d9769cf67a6093cd14fa988a2b8609d00e6fcbd3065e280754fe997aaf2d1571\n",
      "  Stored in directory: /Users/OscarIroh_1/Library/Caches/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: h11, distro, wikipedia, httpcore, httpx, openai\n",
      "Successfully installed distro-1.9.0 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.12.0 wikipedia-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Q2A8TGhKm3i5"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7E9HEMJSX-3T"
   },
   "source": [
    "# 1.) Set up OpenAI and the enviornment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4zwwdkZDYDZN"
   },
   "outputs": [],
   "source": [
    "apikey=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8IiKS0snlpYP"
   },
   "outputs": [],
   "source": [
    "openai.api_key = apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key = openai.api_key\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOXc5_BTm9HP"
   },
   "source": [
    "# 2.) Use the wikipedia api to get a function that pulls in the text of a wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-v7OYamHlrEB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['API_URL',\n",
       " 'BeautifulSoup',\n",
       " 'Decimal',\n",
       " 'DisambiguationError',\n",
       " 'HTTPTimeoutError',\n",
       " 'ODD_ERROR_MESSAGE',\n",
       " 'PageError',\n",
       " 'RATE_LIMIT',\n",
       " 'RATE_LIMIT_LAST_CALL',\n",
       " 'RATE_LIMIT_MIN_WAIT',\n",
       " 'RedirectError',\n",
       " 'USER_AGENT',\n",
       " 'WikipediaException',\n",
       " 'WikipediaPage',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'cache',\n",
       " 'datetime',\n",
       " 'debug',\n",
       " 'donate',\n",
       " 'exceptions',\n",
       " 'geosearch',\n",
       " 'languages',\n",
       " 'page',\n",
       " 'random',\n",
       " 're',\n",
       " 'requests',\n",
       " 'search',\n",
       " 'set_lang',\n",
       " 'set_rate_limiting',\n",
       " 'set_user_agent',\n",
       " 'stdout_encode',\n",
       " 'suggest',\n",
       " 'summary',\n",
       " 'sys',\n",
       " 'time',\n",
       " 'timedelta',\n",
       " 'unicode_literals',\n",
       " 'util',\n",
       " 'wikipedia']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir (wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TgY2FkTdmhTH"
   },
   "outputs": [],
   "source": [
    "page_titles = [\"Aritificial Intelligence\", \"UCLA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Kw5H5jMlmmS3"
   },
   "outputs": [],
   "source": [
    "page_title = page_titles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZF3BiZyXltYO"
   },
   "outputs": [],
   "source": [
    "search_results = wikipedia.search(page_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Ef7yfa2jl0iZ"
   },
   "outputs": [],
   "source": [
    "page = wikipedia.page(search_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page.content\n",
    "\n",
    "def get_wikipedia_content(page_title):\n",
    "    search_results = wikipedia.search(page_title)\n",
    "    page = wikipedia.page(search_results[0])\n",
    "    return(page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = get_wikipedia_content(page_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9aruncMmubX"
   },
   "source": [
    "# 3.) Build a chatgpt bot that will analyze the text given and try to locate any false info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Bmai3B6Dmw3O"
   },
   "outputs": [],
   "source": [
    "chat_completions = client.chat.completions.create(\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'I will be giving you an article. I am looking for false information. Please concisely list only the false information found'},\n",
    "        {'role': 'user', 'content': content[:8180]}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1jI-un5PnDjg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The information that a linear network is the simplest kind of feedforward neural network is false, as there are actually simpler types of neural networks.\n",
      "- The statement that the mean squared errors between calculated outputs and given target values are minimized using the method of least squares or linear regression in feedforward neural networks is false.\n"
     ]
    }
   ],
   "source": [
    "print(chat_completions.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_TMKFGN4nDJ4"
   },
   "outputs": [],
   "source": [
    "def chatgpt_error_correction(text):\n",
    "    chat_completions = client.chat.completions.create(\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'I will be giving you an article. I am looking for false information. Please concisely list only the false information found'},\n",
    "        {'role': 'user', 'content': text[:8180]}]\n",
    "    )\n",
    "    print(chat_completions.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FKAJVXSoayA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPw5LyPEobmk"
   },
   "source": [
    "# 4.) Make a for loop and check a few wikipedia pages and return a report of any potentially false info via wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "V7cuhML2ocGn"
   },
   "outputs": [],
   "source": [
    "page_titles = [\"Artificial Intelligence\", \"UCLA\", \"Rain\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________Artificial Intelligence\n",
      "- Funding and interest in AI increased after 2012 when deep learning surpassed all previous AI techniques\n",
      "\n",
      "- Artificial intelligence was founded as an academic discipline in 1956\n",
      "\n",
      "- General intelligence is one of the main long-term goals of AI research\n",
      "\n",
      "- AI went through cycles of optimism and AI winter, followed by the AI spring in the early 2020s\n",
      "\n",
      "- The space of possible future actions and situations for AI agents is typically intractably large\n",
      "_______________UCLA\n",
      "- The academic roots of UCLA were not established in 1881 as a normal school or the southern branch of the California State Normal School\n",
      "- UCLA did not evolve into San José State University\n",
      "- UCLA did not receive 174,914 undergraduate applications for Fall 2022\n",
      "- UCLA does not have 337 undergraduate and graduate degree programs\n",
      "- Stanford University does not have 128 NCAA team titles, making UCLA second in this regard\n",
      "- UCLA has not been represented in every Olympics since its founding (except in 1924)\n",
      "- UCLA has not had a gold medalist in every Olympics in which the U.S. has participated since 1932\n",
      "- UCLA has not had 27 Nobel laureates affiliated with it\n",
      "- UCLA was not officially considered a Public Ivy\n",
      "- The Los Angeles branch of the California State Normal School did not change its name to Los Angeles State Normal School in 1887\n",
      "- The UCLA Lab School was not established as an elementary school in 1882\n",
      "- UCLA did not open its campus in Westwood to students in 1929\n",
      "- UCLA was not treated as an off-site department of the main campus in Berkeley for its first 32 years\n",
      "- UCLA was not formally elevated to co-equal status with UC Berkeley in 1951\n",
      "- The 2016 murder-suicide did not result in two deaths at an engineering building at UCLA\n",
      "_______________Rain\n",
      "- The first steam locomotive was not built by British inventor Richard Trevithick in 1804, but rather by Richard Trevithick in 1802.\n",
      "- The first steam railroad did not open in the United States in 1829, but rather in the United Kingdom.\n",
      "- Train technology did not start to improve in Germany as mentioned, but rather in various countries around the world simultaneously.\n",
      "- Werner von Siemens did not build the first train powered by electricity in 1879, as electric trains were in operation before that date.\n",
      "- Rudolf Diesel did not construct the first diesel engine in the 1890s, as diesel engine prototypes existed prior to his work.\n",
      "- The \"Flying Hamburger\" was not developed in Germany in 1933, but rather in 1932.\n",
      "- Italy did not develop an extensive network of electric trains driven by a lack of coal reserves in the first decades of the 20th century.\n",
      "- The majority of the world's steam locomotives were not retired by 1980; many were still in operation in various parts of the world.\n",
      "- China was not the last country to fully dieselize, as other countries continued to use steam locomotives beyond 2005.\n",
      "- High-speed rail service did not start with the Japanese Shinkansen in 1964, as high-speed rail service had been developed and operating in other countries before then.\n"
     ]
    }
   ],
   "source": [
    "for page_title in page_titles:\n",
    "    try: \n",
    "        print(\"_______________\" + page_title)\n",
    "        content = get_wikipedia_content(page_title)\n",
    "        chatgpt_error_correction(content)\n",
    "    except:\n",
    "        print(\"ERROR\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
